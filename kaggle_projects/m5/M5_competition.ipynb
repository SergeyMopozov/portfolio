{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle M5 competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "lib_path = '../../../frameworks/time_series_signals_sound'\n",
    "sys.path.append(lib_path)\n",
    "lib_path_2 = '../../../frameworks/'\n",
    "sys.path.append(lib_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import ts_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data/kaggle/M5/sales_train_evaluation.csv',\n",
       " '../../../data/kaggle/M5/sell_prices.csv',\n",
       " '../../../data/kaggle/M5/calendar.csv',\n",
       " '../../../data/kaggle/M5/sales_train_validation.csv',\n",
       " '../../../data/kaggle/M5/sample_submission.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('../../../data/kaggle/M5/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('../../../data/kaggle/M5/calendar.csv')\n",
    "#sales = pd.read_csv('../../../data/kaggle/M5/sales_train_evaluation.csv')\n",
    "#prices = pd.read_csv('../../../data/kaggle/M5/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          1969 non-null   object\n",
      " 1   wm_yr_wk      1969 non-null   int64 \n",
      " 2   weekday       1969 non-null   object\n",
      " 3   wday          1969 non-null   int64 \n",
      " 4   month         1969 non-null   int64 \n",
      " 5   year          1969 non-null   int64 \n",
      " 6   d             1969 non-null   object\n",
      " 7   event_name_1  162 non-null    object\n",
      " 8   event_type_1  162 non-null    object\n",
      " 9   event_name_2  5 non-null      object\n",
      " 10  event_type_2  5 non-null      object\n",
      " 11  snap_CA       1969 non-null   int64 \n",
      " 12  snap_TX       1969 non-null   int64 \n",
      " 13  snap_WI       1969 non-null   int64 \n",
      "dtypes: int64(7), object(7)\n",
      "memory usage: 215.5+ KB\n"
     ]
    }
   ],
   "source": [
    "calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   store_id    object \n",
      " 1   item_id     object \n",
      " 2   wm_yr_wk    int64  \n",
      " 3   sell_price  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 208.8+ MB\n"
     ]
    }
   ],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add future\n",
    "for d in range(1942,1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales[col] = 0\n",
    "    sales[col] = sales[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales = downcast(sales)\n",
    "#prices = downcast(prices)\n",
    "calendar = downcast(calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# sales_valid = pd.read_csv('../../../data/kaggle/M5/sales_train_validation.csv')\n",
    "# sample = pd.read_csv('../../../data/kaggle/M5/sample_submission.csv', index_col=0)\n",
    "# valid = []\n",
    "# for idx, row in sales_valid.iterrows():\n",
    "#     valid.append(ts_models.baseline_forecast(row[6:], \n",
    "#                                        ftype='season mean', \n",
    "#                                        steps=28, \n",
    "#                                        n_seasons=4, \n",
    "#                                        time_index=False,\n",
    "#                                        index=sample.columns).to_frame().T)\n",
    "\n",
    "# valid = pd.concat(valid).reset_index(drop=True)\n",
    "# valid['id'] = data_valid['id']\n",
    "    \n",
    "    \n",
    "# evalut = []\n",
    "# for idx, row in sales.iterrows():\n",
    "#     evalut.append(ts_models.baseline_forecast(row[6:], \n",
    "#                                        ftype='season mean', \n",
    "#                                        steps=28, \n",
    "#                                        n_seasons=4, \n",
    "#                                        time_index=False,\n",
    "#                                        index=sample.columns).to_frame().T)\n",
    "\n",
    "# evalut = pd.concat(evalut).reset_index(drop=True)\n",
    "# evalut['id'] = data['id']\n",
    "    \n",
    "# submission =  pd.concat([valid, evalut]).set_index('id')\n",
    "# submission.to_csv('baselin_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 43.7 s, total: 2min\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# melt series and merge all data\n",
    "df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()\n",
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del sales, prices, calendar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 563 ms, sys: 97.3 ms, total: 660 ms\n",
      "Wall time: 689 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['enc'+'item_id'+'mean'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sold</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>encitem_idmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30490</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_2</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60980</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_3</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91470</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_4</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121960</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_5</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59882360</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.382812</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59912850</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.382812</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59943340</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.382812</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59973830</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.382812</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60004320</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "      <td>Father's day</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.382812</td>\n",
       "      <td>0.216553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id        item_id    dept_id   cat_id  \\\n",
       "0         HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "30490     HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "60980     HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "91470     HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "121960    HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "...                                 ...            ...        ...      ...   \n",
       "59882360  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "59912850  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "59943340  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "59973830  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "60004320  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "\n",
       "         store_id state_id       d  sold       date  wm_yr_wk  ...  year  \\\n",
       "0            CA_1       CA     d_1     0 2011-01-29     11101  ...  2011   \n",
       "30490        CA_1       CA     d_2     0 2011-01-30     11101  ...  2011   \n",
       "60980        CA_1       CA     d_3     0 2011-01-31     11101  ...  2011   \n",
       "91470        CA_1       CA     d_4     0 2011-02-01     11101  ...  2011   \n",
       "121960       CA_1       CA     d_5     0 2011-02-02     11101  ...  2011   \n",
       "...           ...      ...     ...   ...        ...       ...  ...   ...   \n",
       "59882360     CA_1       CA  d_1965     0 2016-06-15     11620  ...  2016   \n",
       "59912850     CA_1       CA  d_1966     0 2016-06-16     11620  ...  2016   \n",
       "59943340     CA_1       CA  d_1967     0 2016-06-17     11620  ...  2016   \n",
       "59973830     CA_1       CA  d_1968     0 2016-06-18     11621  ...  2016   \n",
       "60004320     CA_1       CA  d_1969     0 2016-06-19     11621  ...  2016   \n",
       "\n",
       "          event_name_1  event_type_1  event_name_2 event_type_2 snap_CA  \\\n",
       "0                  NaN           NaN           NaN          NaN       0   \n",
       "30490              NaN           NaN           NaN          NaN       0   \n",
       "60980              NaN           NaN           NaN          NaN       0   \n",
       "91470              NaN           NaN           NaN          NaN       1   \n",
       "121960             NaN           NaN           NaN          NaN       1   \n",
       "...                ...           ...           ...          ...     ...   \n",
       "59882360           NaN           NaN           NaN          NaN       0   \n",
       "59912850           NaN           NaN           NaN          NaN       0   \n",
       "59943340           NaN           NaN           NaN          NaN       0   \n",
       "59973830           NaN           NaN           NaN          NaN       0   \n",
       "60004320  NBAFinalsEnd      Sporting  Father's day     Cultural       0   \n",
       "\n",
       "         snap_TX snap_WI  sell_price  encitem_idmean  \n",
       "0              0       0         NaN        0.216553  \n",
       "30490          0       0         NaN        0.216553  \n",
       "60980          0       0         NaN        0.216553  \n",
       "91470          1       0         NaN        0.216553  \n",
       "121960         0       1         NaN        0.216553  \n",
       "...          ...     ...         ...             ...  \n",
       "59882360       1       1    8.382812        0.216553  \n",
       "59912850       0       0    8.382812        0.216553  \n",
       "59943340       0       0    8.382812        0.216553  \n",
       "59973830       0       0    8.382812        0.216553  \n",
       "60004320       0       0    8.382812        0.216553  \n",
       "\n",
       "[1969 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id'] == 'HOBBIES_1_001_CA_1_evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60034810 entries, 0 to 60034809\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   id            category      \n",
      " 1   item_id       category      \n",
      " 2   dept_id       category      \n",
      " 3   cat_id        category      \n",
      " 4   store_id      category      \n",
      " 5   state_id      category      \n",
      " 6   d             object        \n",
      " 7   sold          int16         \n",
      " 8   date          datetime64[ns]\n",
      " 9   wm_yr_wk      int16         \n",
      " 10  weekday       category      \n",
      " 11  wday          int8          \n",
      " 12  month         int8          \n",
      " 13  year          int16         \n",
      " 14  event_name_1  category      \n",
      " 15  event_type_1  category      \n",
      " 16  event_name_2  category      \n",
      " 17  event_type_2  category      \n",
      " 18  snap_CA       int8          \n",
      " 19  snap_TX       int8          \n",
      " 20  snap_WI       int8          \n",
      " 21  sell_price    float16       \n",
      "dtypes: category(11), datetime64[ns](1), float16(1), int16(3), int8(5), object(1)\n",
      "memory usage: 2.8+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dict \n",
    "d_id = dict(zip(df.id.cat.codes, df.id))\n",
    "d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event coumns preprocessing\n",
    "list1=['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "for i in list1:\n",
    "    \n",
    "    df[i] = df[i].cat.add_categories(\"nan\").fillna(\"nan\")\n",
    "    df[i]=LabelEncoder().fit_transform(df[i]).astype(np.int8)\n",
    "    df[i]=df[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract days\n",
    "df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.dtypes.index.tolist()\n",
    "types = df.dtypes.values.tolist()\n",
    "for i,type in enumerate(types):\n",
    "    if type.name == 'category':\n",
    "        df[cols[i]] = df[cols[i]].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].apply(lambda x: x.strftime('%d')).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60034810 entries, 0 to 60034809\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   id            int16  \n",
      " 1   item_id       int16  \n",
      " 2   dept_id       int8   \n",
      " 3   cat_id        int8   \n",
      " 4   store_id      int8   \n",
      " 5   state_id      int8   \n",
      " 6   d             int16  \n",
      " 7   sold          int16  \n",
      " 8   date          int8   \n",
      " 9   wm_yr_wk      int16  \n",
      " 10  weekday       int8   \n",
      " 11  wday          int8   \n",
      " 12  month         int8   \n",
      " 13  year          int16  \n",
      " 14  event_name_1  int8   \n",
      " 15  event_type_1  int8   \n",
      " 16  event_name_2  int8   \n",
      " 17  event_type_2  int8   \n",
      " 18  snap_CA       int8   \n",
      " 19  snap_TX       int8   \n",
      " 20  snap_WI       int8   \n",
      " 21  sell_price    float16\n",
      "dtypes: float16(1), int16(6), int8(15)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 3.41 s, total: 25.2 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#lags sold\n",
    "lags = [28,35,42,49]\n",
    "for lag in lags:\n",
    "    df['sold_lag_'+str(lag)] = df.groupby(['id', \n",
    "                                           'item_id', \n",
    "                                           'dept_id',\n",
    "                                           'cat_id', \n",
    "                                           'store_id', \n",
    "                                           'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags event\n",
    "lags2 = [1,2]\n",
    "for lag in lags2:\n",
    "    df['event1_lag_'+str(lag)] = df.groupby(['id', \n",
    "                                             'item_id', \n",
    "                                             'dept_id', \n",
    "                                             'cat_id', \n",
    "                                             'store_id', \n",
    "                                             'state_id'],as_index=False)['event_name_1']\\\n",
    "                                    .shift(lag)\\\n",
    "                                    .astype(np.float16)\n",
    "    \n",
    "    df['event1_lag_'+str(lag)].fillna(100, inplace=True)\n",
    "    df['event1_lag_'+str(lag)] = df['event1_lag_'+str(lag)].astype(np.int8)\n",
    "    df['event1_lag_'+str(lag)] = df['event1_lag_'+str(lag)].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 2.05 s, total: 15 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# mean sold of item by all stores\n",
    "df['item_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)    \n",
    "#df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\n",
    "#df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\n",
    "\n",
    "# mean sold  by category, department, cat + dep, store + item, cat + item, dep+item, store+cat+dep\n",
    "df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
    "#df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\n",
    "#df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\n",
    "df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.84 s, sys: 1.05 s, total: 3.89 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# replace week number to atarted from zero  \n",
    "df['wm_yr_wk_linear']=LabelEncoder().fit_transform(df['wm_yr_wk'].values).astype(np.int16)\n",
    "df.drop(['wm_yr_wk'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.82 s, sys: 1.12 s, total: 7.94 s\n",
      "Wall time: 7.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# add week price lag and diff\n",
    "df['price_lag'] = df.groupby(['id', \n",
    "                              'item_id', \n",
    "                              'dept_id', \n",
    "                              'cat_id', \n",
    "                              'store_id', \n",
    "                              'state_id'],as_index=False)['sell_price'].shift(7).astype(np.float16)\n",
    "# TODO sell price contain nan price-diff - too\n",
    "df['price-diff']=df['price_lag']-df['sell_price']\n",
    "df.drop(['price_lag'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 1.14 s, total: 18.3 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# add value of cents - 99 effect ?\n",
    "df['sell_price'].fillna(-1,inplace=True)\n",
    "df['decimal']=df['sell_price'].apply(lambda x: 100*(x-int(x))).astype(np.int16)\n",
    "df['sell_price'].replace(-1,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.7 s, sys: 16.2 s, total: 55.8 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calculate expanding mean price with window 2\n",
    "df['expanding_price_mean'] = df.groupby(['id', \n",
    "                                         'item_id', \n",
    "                                         'dept_id', \n",
    "                                         'cat_id', \n",
    "                                         'store_id', \n",
    "                                         'state_id'])['sell_price']\\\n",
    "                                .transform(lambda x: x.expanding(2).mean())\\\n",
    "                                .astype(np.float16)\n",
    "# calculate difference\n",
    "df['diff_moving_mean']=df['expanding_price_mean']-df['sell_price']\n",
    "\n",
    "# drop expanding mean\n",
    "df.drop(['expanding_price_mean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 384 ms, total: 2.93 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# optimize mamory\n",
    "df['price-diff']=df['price-diff'].astype(np.float16)\n",
    "# drop wdas - same as weekday\n",
    "df.drop(['wday'], axis=1, inplace=True)\n",
    "df['decimal']=df['decimal'].astype(np.int8)\n",
    "# encode year\n",
    "df['year']=LabelEncoder().fit_transform(df['year']).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 s, sys: 22.6 s, total: 1min 2s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ?? sell price  not change - may be erros\n",
    "df['daily_avg_sold'] = df.groupby(['id', \n",
    "                                   'item_id', \n",
    "                                   'dept_id', \n",
    "                                   'cat_id', \n",
    "                                   'store_id', \n",
    "                                   'state_id','d'])['sell_price'].transform('mean').astype(np.float16)\n",
    "# mean price\n",
    "df['avg_sold'] = df.groupby(['id', \n",
    "                             'item_id', \n",
    "                             'dept_id', \n",
    "                             'cat_id', \n",
    "                             'store_id', \n",
    "                             'state_id'])['sell_price'].transform('mean').astype(np.float16)\n",
    "# difference between daily price and avg price (may be mean sells)\n",
    "df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\n",
    "df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 936 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#max price\n",
    "df['price_max'] = df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "#df['price_min'] = df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "#df['price_std'] = df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "#df['price_mean'] = df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# relate to max price\n",
    "df['price_norm'] = df['sell_price']/df['price_max']\n",
    "\n",
    "# relate price to mean by month and year\n",
    "#df['price_momentum'] = df['sell_price']/df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "df['price_momentum_m'] = df['sell_price']/df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "df['price_momentum_y'] = df['sell_price']/df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
    "#df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)\n",
    "#df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\n",
    "#df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\n",
    "#df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\n",
    "#df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop category and state\n",
    "list3=['cat_id','state_id']\n",
    "for i in list3:\n",
    "    df.drop([i], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preapare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 4.06 s, total: 16 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df[df['d']>=49]\n",
    "df.to_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data on train/valid/test sets\n",
    "valid_csv=data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\n",
    "test = data[data['d']>=1942][['id','d','sold']]\n",
    "eval_preds = test['sold']\n",
    "valid_preds_csv=valid_csv['sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_column=[]\n",
    "for i in data.columns:\n",
    "    if(str(data.dtypes[i])=='category'):\n",
    "        cat_column.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Prediction for Store: CA_1*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 2.42529\ttraining's tweedie: 3.36887\tvalid_1's rmse: 2.28445\tvalid_1's tweedie: 4.30605\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's rmse: 2.37201\ttraining's tweedie: 3.35605\tvalid_1's rmse: 2.26755\tvalid_1's tweedie: 4.30284\n",
      "*****Prediction for Store: CA_2*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 1.85687\ttraining's tweedie: 2.8356\tvalid_1's rmse: 2.11245\tvalid_1's tweedie: 4.39203\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's rmse: 1.83072\ttraining's tweedie: 2.82176\tvalid_1's rmse: 2.08933\tvalid_1's tweedie: 4.38725\n",
      "*****Prediction for Store: CA_3*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 3.375\ttraining's tweedie: 3.95914\tvalid_1's rmse: 2.63403\tvalid_1's tweedie: 4.85689\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's rmse: 3.23963\ttraining's tweedie: 3.93674\tvalid_1's rmse: 2.62072\tvalid_1's tweedie: 4.85093\n",
      "*****Prediction for Store: CA_4*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 1.40118\ttraining's tweedie: 2.48667\tvalid_1's rmse: 1.45656\tvalid_1's tweedie: 3.37453\n",
      "Early stopping, best iteration is:\n",
      "[129]\ttraining's rmse: 1.38539\ttraining's tweedie: 2.47653\tvalid_1's rmse: 1.45075\tvalid_1's tweedie: 3.3715\n",
      "*****Prediction for Store: TX_1*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's rmse: 2.13482\ttraining's tweedie: 2.80948\tvalid_1's rmse: 1.93106\tvalid_1's tweedie: 3.52311\n",
      "*****Prediction for Store: TX_2*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's rmse: 2.55055\ttraining's tweedie: 3.19703\tvalid_1's rmse: 2.01742\tvalid_1's tweedie: 3.87778\n",
      "*****Prediction for Store: TX_3*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 2.09914\ttraining's tweedie: 2.86406\tvalid_1's rmse: 2.13239\tvalid_1's tweedie: 3.897\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's rmse: 2.11413\ttraining's tweedie: 2.86812\tvalid_1's rmse: 2.13937\tvalid_1's tweedie: 3.89487\n",
      "*****Prediction for Store: WI_1*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 1.58942\ttraining's tweedie: 2.76705\tvalid_1's rmse: 1.77274\tvalid_1's tweedie: 4.00022\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's rmse: 1.584\ttraining's tweedie: 2.76384\tvalid_1's rmse: 1.77026\tvalid_1's tweedie: 3.99934\n",
      "*****Prediction for Store: WI_2*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's rmse: 2.63258\ttraining's tweedie: 2.9317\tvalid_1's rmse: 3.19494\tvalid_1's tweedie: 4.26425\n",
      "*****Prediction for Store: WI_3*****\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\ttraining's rmse: 2.29453\ttraining's tweedie: 2.88632\tvalid_1's rmse: 2.13331\tvalid_1's tweedie: 3.74426\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's rmse: 2.25345\ttraining's tweedie: 2.87743\tvalid_1's rmse: 2.10947\tvalid_1's tweedie: 3.74094\n",
      "CPU times: user 2h 38min 37s, sys: 18.3 s, total: 2h 38min 55s\n",
      "Wall time: 15min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for store in d_store_id:\n",
    "    # filter data by store\n",
    "    df = data[data['store_id']==store]\n",
    "    \n",
    "    # split train/valid/test sets\n",
    "    X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\n",
    "    X_valid_csv, y_valid_csv = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\n",
    "    X_test = df[df['d']>=1942].drop('sold',axis=1)\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "        learning_rate= 0.05,\n",
    "        subsample=0.6,\n",
    "        feature_fraction=0.6,\n",
    "        num_iterations = 1200,\n",
    "        max_bin=350,\n",
    "        num_leaves= 300,\n",
    "        lambda_l2=0.003,\n",
    "        max_depth=200,\n",
    "        min_data_in_leaf= 80,\n",
    "        force_row_wise= True, #objective='tweedie'\n",
    "    )\n",
    "    #fit model\n",
    "    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid_csv,y_valid_csv)],\n",
    "             eval_metric='rmse',  verbose=100, early_stopping_rounds=20,categorical_feature=cat_column)\n",
    "    #predict valid values\n",
    "    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)\n",
    "    # predict x_test\n",
    "    eval_preds[X_test.index] = model.predict(X_test)\n",
    "    filename = 'model'+str(d_store_id[store])+'.pkl'\n",
    "\n",
    "    joblib.dump(model, filename)\n",
    "    del model, X_train, y_train, X_valid_csv, y_valid_csv\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv['sold'] = valid_preds_csv\n",
    "validation = valid_csv[['id','d','sold']]\n",
    "validation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index()\n",
    "validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "validation.id = validation.id.map(d_id).str.replace('evaluation','validation')\n",
    "\n",
    "test['sold'] = eval_preds\n",
    "evaluation = test[['id','d','sold']]\n",
    "evaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\n",
    "evaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "evaluation.id = evaluation.id.map(d_id)\n",
    "\n",
    "\n",
    "submit = pd.concat([validation,evaluation]).reset_index(drop=True)\n",
    "\n",
    "# increase on 4% ??\n",
    "for i in range(1,29):\n",
    "    submit['F'+str(i)] *= 1.04\n",
    "\n",
    "submit.to_csv('submission_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = data[data['d']>=1942]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                0\n",
       "item_id                           0\n",
       "dept_id                           0\n",
       "store_id                          0\n",
       "d                                 0\n",
       "sold                              0\n",
       "date                              0\n",
       "weekday                           0\n",
       "month                             0\n",
       "year                              0\n",
       "event_name_1                      0\n",
       "event_type_1                      0\n",
       "event_name_2                      0\n",
       "event_type_2                      0\n",
       "snap_CA                           0\n",
       "snap_TX                           0\n",
       "snap_WI                           0\n",
       "sell_price                 11439643\n",
       "sold_lag_28                       0\n",
       "sold_lag_35                       0\n",
       "sold_lag_42                       0\n",
       "sold_lag_49                   30490\n",
       "event1_lag_1                      0\n",
       "event1_lag_2                      0\n",
       "item_sold_avg                     0\n",
       "cat_sold_avg                      0\n",
       "dept_sold_avg                     0\n",
       "cat_dept_sold_avg                 0\n",
       "store_item_sold_avg               0\n",
       "cat_item_sold_avg                 0\n",
       "dept_item_sold_avg                0\n",
       "store_cat_dept_sold_avg           0\n",
       "wm_yr_wk_linear                   0\n",
       "price-diff                 11558133\n",
       "decimal                           0\n",
       "diff_moving_mean           11456532\n",
       "selling_trend              11439643\n",
       "price_max                         0\n",
       "price_norm                 11439643\n",
       "price_momentum_m           11439643\n",
       "price_momentum_y           11439643\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "item_id                    0\n",
       "dept_id                    0\n",
       "store_id                   0\n",
       "d                          0\n",
       "sold                       0\n",
       "date                       0\n",
       "weekday                    0\n",
       "month                      0\n",
       "year                       0\n",
       "event_name_1               0\n",
       "event_type_1               0\n",
       "event_name_2               0\n",
       "event_type_2               0\n",
       "snap_CA                    0\n",
       "snap_TX                    0\n",
       "snap_WI                    0\n",
       "sell_price                 0\n",
       "sold_lag_28                0\n",
       "sold_lag_35                0\n",
       "sold_lag_42                0\n",
       "sold_lag_49                0\n",
       "event1_lag_1               0\n",
       "event1_lag_2               0\n",
       "item_sold_avg              0\n",
       "cat_sold_avg               0\n",
       "dept_sold_avg              0\n",
       "cat_dept_sold_avg          0\n",
       "store_item_sold_avg        0\n",
       "cat_item_sold_avg          0\n",
       "dept_item_sold_avg         0\n",
       "store_cat_dept_sold_avg    0\n",
       "wm_yr_wk_linear            0\n",
       "price-diff                 0\n",
       "decimal                    0\n",
       "diff_moving_mean           0\n",
       "selling_trend              0\n",
       "price_max                  0\n",
       "price_norm                 0\n",
       "price_momentum_m           0\n",
       "price_momentum_y           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['d']<1914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                               0\n",
       "item_id                          0\n",
       "dept_id                          0\n",
       "store_id                         0\n",
       "d                                0\n",
       "sold                             0\n",
       "date                             0\n",
       "weekday                          0\n",
       "month                            0\n",
       "year                             0\n",
       "event_name_1                     0\n",
       "event_type_1                     0\n",
       "event_name_2                     0\n",
       "event_type_2                     0\n",
       "snap_CA                          0\n",
       "snap_TX                          0\n",
       "snap_WI                          0\n",
       "sell_price                 1063518\n",
       "sold_lag_28                      0\n",
       "sold_lag_35                      0\n",
       "sold_lag_42                      0\n",
       "sold_lag_49                   3049\n",
       "event1_lag_1                     0\n",
       "event1_lag_2                     0\n",
       "item_sold_avg                    0\n",
       "cat_sold_avg                     0\n",
       "dept_sold_avg                    0\n",
       "cat_dept_sold_avg                0\n",
       "store_item_sold_avg              0\n",
       "cat_item_sold_avg                0\n",
       "dept_item_sold_avg               0\n",
       "store_cat_dept_sold_avg          0\n",
       "wm_yr_wk_linear                  0\n",
       "price-diff                 1074822\n",
       "decimal                          0\n",
       "diff_moving_mean           1065129\n",
       "selling_trend              1063518\n",
       "price_max                        0\n",
       "price_norm                 1063518\n",
       "price_momentum_m           1063518\n",
       "price_momentum_y           1063518\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MOON PHASE CALCULATION\n",
    "credits to: https://gist.github.com/miklb/ed145757971096565723\n",
    "moonphase.py - Calculate Lunar Phase\n",
    "Author: Sean B. Palmer, inamidst.com\n",
    "Cf. http://en.wikipedia.org/wiki/Lunar_phase#Lunar_phase_calculation\n",
    "\"\"\"\n",
    "import math, decimal\n",
    "dec = decimal.Decimal\n",
    "\n",
    "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "    diff = d - datetime(2001, 1, 1)\n",
    "    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
    "    return int(phase_index) &amp; 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
