{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "def MPE(y_true, y_pred): \n",
    "    return np.mean((y_true - y_pred) / y_true) * 100\n",
    "\n",
    "\n",
    "\n",
    "def weighted_MAPE(model, X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    result = []\n",
    "    for idx in X_train['id'].unique():\n",
    "        \n",
    "        idx_train = X_train[X_train['id'] == idx].index\n",
    "        idx_test = X_test[X_test['id'] == idx].index\n",
    "        \n",
    "        X_train_id = X_train_scaled[idx_train]\n",
    "        X_test_id = X_test_scaled[idx_test]\n",
    "\n",
    "        y_train_id = y_train[idx_train]\n",
    "        y_test_id = y_test[idx_test]\n",
    "\n",
    "        train_prediction = model.predict(X_train_id)\n",
    "        train_error = mean_absolute_percentage_error(train_prediction, y_train_id)\n",
    "        \n",
    "        test_prediction = model.predict(X_test_id)\n",
    "        test_error_abs = mean_absolute_percentage_error(test_prediction, y_test_id)\n",
    "        test_error_mean = mean_percentage_error(test_prediction, y_test_id)\n",
    "        \n",
    "        \n",
    "        positive_err_rate  = sum((test_prediction - y_test_id) > 0)/ len(y_test_id)\n",
    "        \n",
    "        result.append([idx, train_error, test_error_abs, test_error_mean, y_test_id.sum(), positive_err_rate])\n",
    "        \n",
    "        \n",
    "        \n",
    "    result = pd.DataFrame(result, columns=['idx', 'train_error', 'test_error_abs', \n",
    "                                           'test_error_mean','sum_value', 'positive_err_rate'])\n",
    "    result['weight'] = result['sum_value'] / result['sum_value'].sum()\n",
    "    result['test_error_weighted'] = result['weight'] * result['test_error_mean']\n",
    "    result['test_error_abs_weighted'] = result['weight'] * result['test_error_abs']\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MASE(training_series, testing_series, prediction_series):\n",
    "    \"\"\"\n",
    "    Computes the MEAN-ABSOLUTE SCALED ERROR forcast error for univariate time series prediction.\n",
    "    \n",
    "    See \"Another look at measures of forecast accuracy\", Rob J Hyndman\n",
    "    \n",
    "    parameters:\n",
    "        training_series: the series used to train the model, 1d numpy array\n",
    "        testing_series: the test series to predict, 1d numpy array or float\n",
    "        prediction_series: the prediction of testing_series, 1d numpy array (same size as testing_series) or float\n",
    "        absolute: \"squares\" to use sum of squares and root the result, \"absolute\" to use absolute values.\n",
    "    \n",
    "    \"\"\"\n",
    "    print (\"Needs to be tested.\")\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(np.diff( training_series) ).sum()/(n-1)\n",
    "    \n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d\n",
    "\n",
    "\n",
    "def SMAPE(y_true, y_pred): \n",
    "    return np.mean(np.abs(y_pred - y_true)/((np.abs(y_pred) + np.abs(y_true))/2))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index].reset_index(drop=True)\n",
    "    y_train = y.iloc[:test_index].reset_index(drop=True)\n",
    "    X_test = X.iloc[test_index:].reset_index(drop=True)\n",
    "    y_test = y.iloc[test_index:].reset_index(drop=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plotModelResults(model, idx_train, idx_test, X_train, X_test, y_train, y_test, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    X_train = X_train[idx_train]\n",
    "    X_test = X_test[idx_test]\n",
    "    \n",
    "    y_train = y_train[idx_train]\n",
    "    y_test = y_test[idx_test]\n",
    "    \n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    warnings.filterwarnings('default')\n",
    "    \n",
    "\n",
    "def plot_train_ModelResults(model, idx_train, idx_test, \n",
    "                            X_train, X_test, y_train, y_test, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    X_train = X_train[idx_train]\n",
    "    X_test = X_test[idx_test]\n",
    "    \n",
    "    y_train = y_train[idx_train]\n",
    "    y_test = y_test[idx_test]\n",
    "    \n",
    "    train_prediction = model.predict(X_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    train_range = np.arange(len(y_train))\n",
    "    \n",
    "    plt.plot(train_range, train_prediction, \"g\", label=\"prediction_train\",  alpha=0.5)\n",
    "    plt.plot(train_range, y_train.values, label=\"actual_train\", alpha=0.5)\n",
    "    \n",
    "    test_range = np.arange(len(y_test)) +  len(y_train)\n",
    "    plt.plot(test_range, prediction, \"g\", label=\"prediction\", linewidth=2.0, c='r')\n",
    "    plt.plot(test_range, y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(test_range, lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(test_range, upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(test_range, anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    warnings.filterwarnings('default')\n",
    "    \n",
    "    \n",
    "    \n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1).iloc[:25]\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_1_data = pd.read_pickle('../../data/3/data/value_1_data.pckl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4cbd763ab394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeseries_train_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# sort data\n",
    "value_1_data = value_1_data.sort_values(['day', 'id'])\n",
    "\n",
    "# train-test split\n",
    "y = value_1_data.dropna()['value']\n",
    "X = value_1_data.dropna().drop(['day', 'value'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ridge = RidgeCV(cv=tscv)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "plotCoefficients(ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ridge.predict(X_test_scaled)\n",
    "error = mean_absolute_percentage_error(prediction, y_test)\n",
    "print(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "\n",
    "\n",
    "prediction = ridge.predict(X_test_scaled)\n",
    "error = mean_percentage_error(prediction, y_test)\n",
    "print(\"Mean percentage error {0:.2f}%\".format(error))\n",
    "\n",
    "\n",
    "result_ridge = measure_qulity(ridge, X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "error = result_ridge['test_error_weighted'].sum()\n",
    "print(\"Mean weighted absolute percentage error {0:.2f}%\".format(error))\n",
    "display(result_ridge.sort_values('weight', ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "idx = 488\n",
    "idx_train = X_train[X_train['id'] == idx].index\n",
    "idx_test = X_test[X_test['id'] == idx].index\n",
    "plot_train_ModelResults(ridge, idx_train, idx_test,\n",
    "                 X_train_scaled, \n",
    "                 X_test_scaled,\n",
    "                 y_train, \n",
    "                 y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "\n",
    "idx_train = X_train[X_train['id'] == idx].index\n",
    "idx_test = X_test[X_test['id'] == idx].index\n",
    "plotModelResults(ridge, idx_train, idx_test,\n",
    "                 X_train_scaled, \n",
    "                 X_test_scaled,\n",
    "                 y_train, \n",
    "                 y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso = LassoCV(cv=tscv)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "plotCoefficients(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lasso.predict(X_test_scaled)\n",
    "error = mean_absolute_percentage_error(prediction, y_test)\n",
    "print(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "\n",
    "prediction = lasso.predict(X_test_scaled)\n",
    "error = mean_percentage_error(prediction, y_test)\n",
    "print(\"Mean percentage error {0:.2f}%\".format(error))\n",
    "\n",
    "\n",
    "result_lasso = measure_qulity(lasso, X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "error = result_lasso['test_error_weighted'].sum()\n",
    "print(\"Mean weighted absolute percentage error {0:.2f}%\".format(error))\n",
    "display(result_lasso.sort_values('weight', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 488\n",
    "idx_train = X_train[X_train['id'] == idx].index\n",
    "idx_test = X_test[X_test['id'] == idx].index\n",
    "plot_train_ModelResults(lasso, idx_train, idx_test,\n",
    "                 X_train_scaled, \n",
    "                 X_test_scaled,\n",
    "                 y_train, \n",
    "                 y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "\n",
    "\n",
    "idx_train = X_train[X_train['id'] == idx].index\n",
    "idx_test = X_test[X_test['id'] == idx].index\n",
    "plotModelResults(lasso, idx_train, idx_test,\n",
    "                 X_train_scaled, \n",
    "                 X_test_scaled,\n",
    "                 y_train, \n",
    "                 y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm_model = LGBMRegressor(n_estimators=1000, )\n",
    "lgbm_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lgbm_model.predict(X_test_scaled)\n",
    "error = mean_absolute_percentage_error(prediction, y_test)\n",
    "print(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "\n",
    "prediction = lgbm_model.predict(X_test_scaled)\n",
    "error = mean_percentage_error(prediction, y_test)\n",
    "print(\"Mean percentage error {0:.2f}%\".format(error))\n",
    "\n",
    "\n",
    "result_lgbm = measure_qulity(lgbm_model, X_train, X_test, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "error = result_lgbm['test_error_weighted'].sum()\n",
    "print(\"Mean weighted absolute percentage error {0:.2f}%\".format(error))\n",
    "display(result_lgbm.sort_values('weight', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 488\n",
    "idx_train = X_train[X_train['id'] == idx].index\n",
    "idx_test = X_test[X_test['id'] == idx].index\n",
    "plot_train_ModelResults(lgbm_model, idx_train, idx_test,\n",
    "                 X_train_scaled, \n",
    "                 X_test_scaled,\n",
    "                 y_train, \n",
    "                 y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "\n",
    "idx_train = X_train[X_train['id'] == idx].index\n",
    "idx_test = X_test[X_test['id'] == idx].index\n",
    "plotModelResults(lgbm_model, idx_train, idx_test,\n",
    "                 X_train_scaled, \n",
    "                 X_test_scaled,\n",
    "                 y_train, \n",
    "                 y_test,\n",
    "                 plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
